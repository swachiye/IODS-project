
**Principal Component Analysis **

```{r}
human <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human2.txt", sep=",", header=TRUE)

# Accesss necessary library
library(dplyr)
library(ggplot2)
library(GGally)

```

Data structure

```{r}
# Structure 
str(human)

# Dimension
dim(human)
head(human)

```

Summarise Data

```{r}
summary(human)
```


```{r}
# glimpse at the human data
glimpse(human)

```
  
Explore relationships

```{r}
# Access GGally
library(GGally)

# To visualise the correlation between the variables in our data
ggpairs(human)

```


The diagonal plots give the density plot. The scale of the y axis is printed on the left side of the matrix and x axis is at the bottom of the matrix. 

The upper part of the density plot gives the pearsonâ€™s correlation coefficient of determination (R2) showing positive and negative (-) correlations between the variable in the human development Index. 

Variable that are significantly positive or significantly negative are give by the symbol (***)

The plot below the density plot shows the scatter plot between two variables


We can also look at the correlations using the cor from the *corrplot function*

```{r}
# Access corrplot
library(corrplot)

# compute the correlation matrix and visualize it with corrplot
cor(human) %>% corrplot

```



**Principal Component Analysis (PCA) **

Principal Component Analysis a method used to reduce the dimensionality of large data sets into a smaller one that still contains most of the information in the large set.

The number of PCs is equal to the number of original variables and for our case we have eight (8).

The aim is to keep only the PCs that explain the most variance of our data to make the interpretation easier.


**PCA on Unstandardise data**
```{r}
# perform principal component analysis (with the SVD method)
pca_human1 <- prcomp(human)
summary(pca_human1)

# draw a biplot of the principal component representation and the original variables
biplot(pca_human1, choices = 1:2, cex = c(0.8, 1),col = c("grey30", "red"))
```

```{r}

# Access the factoextra 
library(factoextra)

#Visualize eigenvalues 
fviz_eig(pca_human1)


# Another way to draw a biplot of the principal component
# Gives percentage variance of PC1 on the x axis and PC2 on the y axis
fviz_pca_biplot(pca_human1, repel = TRUE, col.var = "#2E8FDF", # Variables color
                col.ind = "#696959"  # Individuals color
                )

eig.val <- get_eigenvalue(pca_human1)
eig.val
```

Standardization is a advisable for data transformation especially because the variables in original dataset have different scale or different units. 

The standardized variables will therefore be unitless and have a similar variance.

This is necessary to eliminate biases in the original variables. 



**PCA on Standardise data**

```{r}
# standardize the variables
human_std <- scale(human)

# perform principal component analysis (with the SVD method)
pca_human2 <- prcomp(human_std)

#The summary shows outputs the proportion of variance explained by the components.
summary(pca_human2)

# Access the factoextra 
library(factoextra)

#Get eigenvalues to determine the number of principal components to be considered.
eig.val <- get_eigenvalue(pca_human2)
eig.val

#Visualize eigenvalues
#Scree plot,

fviz_eig(pca_human2)
```
The PCA results for standardised data are are different from unstandardized data.

PCA on standardised data are easier to interprete and see the cooreltion as compared to unstandardised data.

Based on these results

The PC1 account for 53.6% of the total variance and the PC2 16.2%. Thus PC1 and PC2 contributes about 69% of the total variation in the dataset and have eigenvalues > 1 
This mean, PC1 and PC2 provides a good approximation of the variation present in the original human dataset that has 8 variable.


```{r}
# create and print out a summary of pca_human
s <- summary(pca_human2)

# rounded percentages of variance captured by each PC
pca_pr <- round(100*s$importance[2,], digits = 1) 

# print out the percentages of variance
pca_pr

# create object pc_lab to be used as axis labels
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")

# draw a biplot of the principal component representation and the original variables
biplot(pca_human2, cex = c(0.8, 1), col = c("grey40", "red"), xlab = pc_lab[1], ylab = pc_lab[2])



# Anothe way to draw a biplot of the principal component
# Gives percentage variance of PC1 on the x axis and PC2 on the y axis
fviz_pca_biplot(pca_human2, repel = TRUE, col.var = "#2E5FDF", # Variables color
                col.ind = "#696959"  # Individuals color
                )
```

From the biplot, 
we can see the variables D and E are highly associated and forms cluster (gene expression response in D and E conditions are highly similar). Similarly, A and B are highly associated and forms another cluster (gene expression response in A and B conditions are highly similar but different from other clusters). If the variables are highly associated, the angle between the variable vectors should be as small as possible in the biplot.



**PCA on Standardise data**
```{r}
# Import data 
# Loading FactoMineR
library(FactoMineR)
library (ggplot2)
library(dplyr)
library(tidyr) 

#Load Tea data
data(tea)

# the structure and 
str(tea)

#the dimensions 
dim(tea)

```


Tes sata is data frame (of factors) containing the answers of a questionnaire on tea consumption 300 individuals.

We consider the following columns to answer the following:

The kind of tea taken
How the tea is taken (alone, w/milk, w/lemon, other)
Form of tea used either tea bags, loose tea or both
If sugar is used or not
Where the tea is bought either the supermarket, shops, both)


```{r}
# select columns of interest
new_tea = tea[, c("Tea", "How", "how", "sugar", "where")]
# take a look
head(new_tea)
```

Perform MCA 

```{r}
# apply MCA
mca = MCA(new_tea, graph = FALSE)
```


Visualization and interpretation


```{r}
# table of eigenvalues
mca$eig

# Plot the eigenvalues
fviz_screeplot(mca, addlabels = TRUE, ylim = c(0, 45))

```


```{r}
# column coordinates
head(mca$var$coord)

```
```{r}
# row coordinates
head(mca$ind$coord)

```
The results displays the coordinates of each variable categories in each dimension


To visualize the correlation between variables and MCA principal dimensions
```{r}
# row coordinates
fviz_mca_var(mca, choice = "mca.cor", 
            repel = TRUE, # Avoid text overlapping (slow)
            ggtheme = theme_minimal())
```

The plot above helps to assess the variables that are the most correlated with each of the dimension. 
The squared correlations between variables and the dimensions are used as coordinates.

It can be seen that, the variables "where" and "how" are both correlated to both dimension 1 and 2.
"How" is more correlated with dimension 2.

Note
How- how the tea is taken (alone, w/milk, w/lemon, other)
how- Form of tea used either tea bags, loose tea or both
If sugar is used or not
Where- where the tea is bought either the supermarket, shops, both)



```{r}

plot.MCA(mca, invisible=c("var","quali.sup"), cex=0.8)
plot.MCA(mca, invisible=c("ind","quali.sup"), cex=0.8)
```
