---
title: "Clustering and classification"
author: "Sheila"
date: "18/11/2020"
output: html_document
---
**Load data **

Boston dataset contains data about housing in the area of Boston Mass as collected by the U.S Census Service.
```{r}
# Access MASS library 
library(MASS)

# Load Boston data
data("Boston")
```

```{r}
# explore Boston dataset structure and dimnesions
str(Boston)
dim(Boston)
```
The Boston dataset has 506 rows and 14 columns. 

The summary of the dataset
```{r}
summary(Boston)

# plot matrix of the variables
pairs(Boston)

```

Check the correlations between variables using correlation matrix plot:

```{r}
library(corrplot)
library(tidyverse)

# First calculate the correlation matrix 
corr_matrix<-cor(Boston)%>% round(2)

# print corr_matrix
corr_matrix

# visualize the correlation matrix
corrplot(corr_matrix, method="circle", type = "upper", tl.cex = 1.2) %>% round(2)
```

Correlation Matrices
Correlation of target variable with predictor variables:

1. crim are highly correlated to indu, nox, age, rad, and tax and Istat and negatively to zn, rm,dis and medv
2. rm and lstat are highly correlated with the target variable medv
3. black, dis, rm, chas, zn are positively correlated with medv
4. crim, indus, nox, age, rad, tax, ptratio, lstat are negatively correlated with medv


** Data standardization**

The fact that Boston data contains only numerical values, we can use the function *scale()* to standardize the whole dataset.

```{r}
# center and standardize variables using scale()
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# check the class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
```

Create a categorical variable of the crime rate in the Boston dataset (from the scaled crime rate).

```{r}
# First create a quantile vector of crim 
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime' into "low", "med_low", "med_high" and "high"
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label=c("low", "med_low", "med_high", "high")) 

```


Drop the old crime rate variable from the dataset.

```{r}
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim) %>% round(2)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

#print
boston_scaled 
```


 Divide the dataset to train and test sets, so that 80% of the data belongs to the train set and 20% in the test dataset.
 
```{r}
# Get the number of rows in the Boston dataset and assign it as *n* 
n <- nrow(boston_scaled)

# Select randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# Use the selected to create train set
train_data <- boston_scaled[ind,]

# create test set using the remaining rows
test_data  <- boston_scaled[-ind,]

```

**Linear Discriminant analysis**

```{r}

# linear discriminant analysis
# the dot means all other variables in the data.
lda.fit <- lda(crime ~ ., data = train_data) 
# print the lda.fit object
lda.fit 

```
The “proportion of trace” shown when you print *lda.fit* is the percentage separation achieved by each discriminant function. Thus, the percentage of classification achieved by the first discriminant function (LD1) is 96%, which is very high, While the second (3.3% ) and the third (1.1%) are very low


**Histogram of the LDA Values**

A good way of displaying the results of a linear discriminant analysis is to make a histogram of the values of each discriminant function.

This can be done using the “ldahist()” function in R. 

```{r}
# LD1
lda.fit.values <- predict(lda.fit)
ldahist(data = lda.fit.values$x[,1], g=train_data$crime)

# LD2
lda.fit.values <- predict(lda.fit)
ldahist(data = lda.fit.values$x[,2], g=train_data$crime)

# LD3
lda.fit.values <- predict(lda.fit)
ldahist(data = lda.fit.values$x[,3], g=train_data$crime)
```
With the first discriminant, we get a clean separation between the classes. However, in second and third discriminant we have overlap between the classes.

**Draw the LDA (bi)plot**
```{r}
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train_data$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)
```

Save the crime categories from the test set and then remove the categorical crime variable from the test dataset

```{r}
# save the correct classes from test data
correct_classes <- test_data$crime

# remove the crime variable from test data
test_data <- dplyr::select(test_data, -crime)
test_data
```


** Make predictions**

```{r}

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test_data)

# cross tabulate the results
tab <- table(correct = correct_classes, predicted = lda.pred$class)
tab
```
Results of the 
Under low, 16 were correctly classified

** Model accuracy**

Compute the model accuracy as follow:
```{r}
sum(diag(tab))/sum(tab)
```





