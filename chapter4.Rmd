---
title: "Clustering and classification"
author: "Sheila"
date: "18/11/2020"
output: html_document
---

**Load data **

Boston dataset contains data about housing in the area of Boston Mass as collected by the U.S Census Service.
```{r}
# Access MASS library 
library(MASS)

# Load Boston data
data("Boston")
```


Assess the data
```{r}
# explore Boston dataset structure and dimnesions
str(Boston)
dim(Boston)
```
The Boston dataset has 506 rows and 14 columns. 


Summarise the dataset
```{r}
summary(Boston)

# plot matrix of the variables
pairs(Boston)

```



Asses correlations between variables using correlation matrix plot:

```{r}
library(corrplot)
library(tidyverse)

# First calculate the correlation matrix 
corr_matrix<-cor(Boston)%>% round(2)
# visualize the correlation matrix
corrplot(corr_matrix, method="circle", type = "upper", tl.cex = 1) %>% round(2)
```

Correlation Matrices

Correlation of target variable crime with predictor variables:

1. crim are highly correlated to rad, moderately to to indu, nox, age, tax, aptratio and Istat but negatively to zn, rm,dis and medv

other correlations include 
2. medv is positively correlated with black, dis, rm, chas, zn  medv and negatively correlated to indus, nox, age, rad, tax, ptratio, lstat 
3. The highest positive correlations are between “rad” and “tax”, “indux” and “nox” and negative between “dis” and “age” and “dis” and “nox”.


** Data standardization**

Boston data contains numerical values, thus we can use the function *scale()* to standardize the whole dataset.

```{r}
# center and standardize variables using scale()
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# check the class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
```

Create a categorical variable of the crime rate in the Boston dataset using the scaled crime rate.

```{r}
# First create a quantile vector of crim 
bins <- quantile(boston_scaled$crim)

# create a categorical variable 'crime' into "low", "med_low", "med_high" and "high"
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label=c("low", "med_low", "med_high", "high")) 

```


Drop the old crime rate variable from the dataset.

```{r}
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim) %>% round(2)# round it to 2 decimal places

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

#print
boston_scaled 
```


Split the data into training and test set so that 80% of the data belongs to the train set and 20% in the test dataset.
 
```{r}
# Get the number of rows in the Boston dataset and assign it as *n* 
n <- nrow(boston_scaled)

# Select randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# Use the selected to create train set
train_data <- boston_scaled[ind,]

# create test set using the remaining rows
test_data  <- boston_scaled[-ind,]

```

**Linear Discriminant analysis**

```{r}

# linear discriminant analysis
# the dot means all other variables in the data.
lda.fit <- lda(crime ~ ., data = train_data) 
# print the lda.fit object
lda.fit 

```
The Group means shows the mean of each variable in each group.

Coefficients of linear discriminants provided the linear combination of predictor variables that are used to form the LDA

The “proportion of trace” shown when you print *lda.fit* is the percentage separation achieved by each discriminant function. Thus, the percentage of classification achieved by the first discriminant function (LD1) is 96%, which is very high, While the second (3.3% ) and the third (1.1%) are very low


**Histogram of the LDA Values**

A good way of displaying the results of a linear discriminant analysis is to make a stacked histogram of the values of each discriminant function.

This can be done using the “ldahist()” function in R. 

```{r}
# LD1
lda.fit.values <- predict(lda.fit)
ldahist(data = lda.fit.values$x[,1], g=train_data$crime)

# LD2
lda.fit.values <- predict(lda.fit)
ldahist(data = lda.fit.values$x[,2], g=train_data$crime)

# LD3
lda.fit.values <- predict(lda.fit)
ldahist(data = lda.fit.values$x[,3], g=train_data$crime)
```
With the first discriminant, we get a clean separation between the classes. However, in second and third discriminant we have overlap between the classes.

**Draw the LDA (bi)plot**
```{r}
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train_data$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)
```

Save the crime categories from the test set and then remove the categorical crime variable from the test dataset

```{r}
# save the correct classes from test data
correct_classes <- test_data$crime

# remove the crime variable from test data
test_data <- dplyr::select(test_data, -crime)
```


** Make predictions**

```{r}

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test_data)

# cross tabulate the results
tab <- table(correct = correct_classes, predicted = lda.pred$class)
tab
```
Results of the 
Under low, 13 were correctly classified, while 9 are classified under med_low and 2 under med_high

under med_low, 13 are correctly classified, while 4 are classified under med_high
Med_high have 14 correctly classified
high has all 30 correctly classified

Overall we assess the model accuracy

** Model accuracy**

Compute the model accuracy as follow:
```{r}
sum(diag(tab))/sum(tab)
```
It can be seen that, our model correctly classified 67% of observations.



** Distance measures**
```{r}
# load MASS and Boston
library(MASS)
data('Boston')
```

Standardize the data

```{r}
# center and standardize variables using scale()
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
```

Calculate the distances between the observations. 
```{r}
# euclidean distance matrix (the Euclidean method is the default one)
dist_eu <- dist(boston_scaled)

# look at the summary of the distances
summary(dist_eu )
```
```{r}
# manhattan distance matrix
dist_man <- dist(boston_scaled, method="manhattan")

# look at the summary of the distances
summary(dist_man)
```

Run k-means algorithm on the dataset. 

```{r}
# k-means clustering
km <-kmeans(boston_scaled, centers = 3)

# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)

```


Investigate what is the optimal number of clusters and run the algorithm again.

```{r}
# K-means might produce different results every time thus use function set.seed() to avoid this.
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)

```

Optimal number of clusters in Kmeans is when the value of total WCSS changes radically and in this case we see the optimal number as 2


```{r}

# k-means clustering
km <-kmeans(boston_scaled, centers = 2)

# Store the cluster for each observation directly in the dataset as a column:

boston_cluster <- data.frame(boston_scaled,
  cluster = as.factor(km$cluster))
head(boston_cluster)

```

Visualize the clusters.

```{r}
# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)

```

```{r}
library(factoextra)
km$size
fviz_cluster(km, data = boston_scaled)
```
The first cluster is composed of 329 observations, while the second 177.
