# **Regession and Model Validation**

Author: Sheila Wachiye

Date:   November 5 2020

# *Introduction*
**Regression?**

Regression is a statistical method used to assess the relationship between a dependent variable and one and more independent variables. The dependent variable can be seen as response variable while the independent variables as explanatory variables. 

**Model validation?**
According to wikipedia, a model validation is the task of confirming that the outputs of a statistical model have enough fidelity to the outputs of the data-generating process that the objectives of the investigation can be achieved.

Regression analysis using RMarkdown include the following steps
For this report, we will be using "learning2014" 

View data using the following link

[Data](https://www.rstudio.com)http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/learning2014.txt)

**Load data into Rmarkdown**

Load data by assigning a name. In this case we use "lrn14". Use *read.table* function, to read the file in a table format and creates a data frame. The separator is a comma "," and the file includes a header

```{r}
lrn14 <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/learning2014.txt", sep=",", header=TRUE)
```


**Visualise data **

Data structure and dimensions 

```{r}
# data structure
#
str(lrn14)
 
# data dimensions
# This gives you the number of rows and columns in the data frame
dim(lrn14)
```
Based on the above output
We have 166 observaions which are the rows and 7 variables in the columns

```{r}
# Obtain the first several rows of a data frame using head
head(lrn14) 

# obtain the column names
colnames(lrn14)
 
```

Ceate graphics using *ggplots function*.

Ggplot2 is an implementation of Leland Wilkinson's Grammar of Graphics which is popula for data  visualization.

Access the *gglot2* in Rmarkdown using library
```{r}
library(ggplot2)
```


Plot Exams points against the students attitude

To highlight the distribution of the attitude based on gender, use *col(gender)*. This will assign different colors for each of the gender.
```{r}
# initialize plot with data and aesthetic mapping
p1 <- ggplot(lrn14, aes(x = attitude, y = points, col= gender))

# define the visualization type (points)
p2 <- p1 + geom_point()
# draw the plot 
p2
# add the regression line
p3 <- p2 + geom_smooth(method = "lm")
# draw the plot 
p3
# add title and draw the plot
p4 <- p3 + ggtitle("Student's attitude in relation to exam points")
p4
```

Use scatter plot matrix of the variables in learning2014 to assess the distributions and correlations between the variables.
```{r}
# [-1] excludes the first column (gender)
 pairs(lrn14[-1])

```

** Correlations between the variables in the dataframe**

To assess the strengths and direction of correlation between the variables, create a more advanced plot matrix with *ggpairs()*. This matrix will give the correlation coefficient (R), which is a  measures of strength and direction of a relationship between two variables on a scatterplot. 

The value of R is lies between +1 (postive correlation) and –1( negative correlation). A positive correlation implies that the two variables under consideration vary in the same direction, i.e., if a variable increases the other one increases and if one decreases the other one decreases as well. A negative correlation means that the two variables under consideration vary in opposite directions, that is, if a variable increases the other decreases and vice versa.  
The strenght of the correlations is how close the R is to +1 for a strong positive result or how close to -1 for a strong negative correlation. 

However, a correlation close to 0 indicates no correlation and thus the two variables are independent.
```{r}
# access GGally using library
library(GGally)

# create a more advanced plot matrix with ggpairs() ns
p5 <- ggpairs(lrn14, mapping = aes(), lower = list(combo = wrap("facethist", bins = 20)))

# plot and take note of the correlations
p5
```


With Exam points as the target (dependent) variable. We note that Exams points is positively correlated to Attitude(R= 0.44) and strategic learning (R= 0.15) but negatively correlated to surface learning (R= 0.14)

We select this three variable to include in our multiple regresion model

**Multiple Regression**

Multiple regression is a form of linear regression that permit predictions of the dependent variables with multiple independent variables i.e. the relationship between one dependent variable and two or more independent variables. This is done by basically adding more independent variables to the linear regression equation, with each term representing the impact of a different variable.

Formula: Y = β~0~ + β1~1~X~1~ + β1~2~X~2~ + β1~3~X~3~ + ε.

```{r}
# learning2014 is available

# create a regression model with multiple explanatory variables (Note from the above matrix that shows correlation with Exam Points). Thus we have attutude, stra and surf

Model_1 <- lm(points ~ attitude+ stra+ surf, data = lrn14)

# print out a summary of the model
summary(Model_1)

#This is the output you should receive.
```
* Call: Shows the model formula and variables used to create the model.

* Residuals: Gives the difference between what the model predicted and the actual value of y.  

* Coefficients: Provide the weights that minimize the sum of the square of the errors.  

P-value is the significance level of the t-test (p-value = 3.156e-08), shows that the overall models including attitude, strategic learning and surface learning is statistically signficant in explaining 21% of variations in exams points at confidence level of 95%

However, individually, only attitude is significant (p-value = 1.93e-08) in explaning variation in exam points.

Thus, we only include attitude in our model, simple linear regression

Formula: Y = β~0~ + β1~1~X~1~ + ε.

```{r}
# Example
# a scatter plot of Exam points versus attitude
qplot(attitude, points, data = lrn14) + geom_smooth(method = "lm")

# fit a linear model

Model_2 <- lm(points ~ attitude, data = lrn14)

# print out a summary of the model
summary(Model_2)

#This is the output you should receive.
```
In the above output, we can deduce that the estimate of the intercept β0 = 11.6372, and the estimate of the
slope β1 = 3.5255, 

Therefore, the line of best fit is

Points = 11.6372 + 3.5255 x Attitude

The R2 is 0.1906. 

R2 is a measure of the linear relationship between our predictor variable (Attitude) and our response (points). 

P-value is the significance level of the t-test (p-value = 4.119e-09), thus based on the result, Attitude is statistically signficant in explaining 19% of variations in exams points at confidence level of 95%


**Diagnostic Plots**

Regression diagnostics are used to evaluate the model assumptions.

The model assumptions for linear regression included:

1. Linearity: The relationship between independent variable and the mean of dependent variable is linear.

2. Homoscedasticity: The variance of residual is the same for any value of independent variable.

3. Independence: Observations are independent of each other.

4. Normality: For any fixed value of the independent variable, the dependent variable is normally distributed.

```{r}
# draw diagnostic plots using the plot() function. Choose the plots 1, 2 and 5
par(mfrow = c(2,2)) # Create a 2 x 2 plotting matrix
plot(Model_1, which= c(1, 2, 5))

```


1. Residuals vs Fitted

This plot shows if residuals have non-linear patterns. 

Residuals are measured as follows: residual =  observed y   –   model-predicted y

Therefore, this plot is useful for checking the assumption of linearity and homoscedasticity. 

 a. For the assumption of linearity, the residuals should not be too far away from 0.
 
 b. For the homoscedasticity assumption, we should not have a pattern in the residuals and that they are equally spread around the y = 0 line.

Based on this our plot, we can tell that the residual are even spread around the 0 line and no pattern can be seen. Therefore, our model  has met these assumption a good indication that we don’t have non-linear relationships.

2. Normal Q-Q

This plot shows if residuals are normally distributed. Therefore, we assess if the residuals follow a straight line well or deviate severely. 

Our data observations lie quite well along the 45-degree line in the QQ-plot, so we may assume that normality holds here as much as it divert at the begining.


3. Residuals vs Leverage

This plot helps us to find the influence of an outlier in the linear model. This is because some outliers may shift the linear regression analysis. While some may have outliers, that might not be affect the regression line. This is the case with our results.


























































































