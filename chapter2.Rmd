# **Regession and Model Validation**

Author: Sheila Wachiye

Date:   November 5 2020

# *Introduction*

**Regression**

Regression is a statistical method used to assess the relationship between a dependent variable and one and more independent variables. The dependent variable can be seen as response variable while the independent variables as explanatory variables. 

**Exercise**

Regression analysis using RMarkdown include the following steps
For this report, we will be using "learning2014" 

View data using this link
[](https://www.rstudio.com)http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/learning2014.txt)

**Load data**

Load data by assigning a name. In this case we use "lrn14". 

Use *read.table* function, to read the file in a table format and creates a data frame. The separator is a comma "," and the file includes a header

```{r}
lrn14 <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/learning2014.txt", sep=",", header=TRUE)
```


**Visualise data **
```{r}
# data structure using str() 
# str() function in is used for compactly display the internal structure of the dataframe
str(lrn14)
# It tells us about each variable one by ones below
 
```
```{r}
# data dimensions dim() 
# This gives you the number of rows and columns in the data frame
dim(lrn14)
```
Based on the above output, we have 166 observaions (rows) and 7 variables (columns)

```{r}
# Access the first top six rows in the data frame using head() and the six bottom using tail()
head(lrn14) 

tail(lrn14) 

# obtain the column names
colnames(lrn14)
 
```

Create graphics using *ggplots function*.

Ggplot2 is an implementation of Leland Wilkinson's Grammar of Graphics which is popula for data  visualization.

```{r}
#Access the *gglot2* using library
library(ggplot2)
```

Plot Exams points against students attitude

```{r}
# initialize plot with data and aesthetic mapping
# ighlight the distribution of the attitude based on gender, use *col(gender)*. This will assign different colors for each of the gender.
p1 <- ggplot(lrn14, aes(x = attitude, y = points, col= gender))

# define the visualization type (points)
p2 <- p1 + geom_point()

# draw the plot 
p2

# add the regression line
p3 <- p2 + geom_smooth(method = "lm")

# draw the plot 
p3

# add title and draw the plot
p4 <- p3 + ggtitle("Student's attitude in relation to exam points")

p4
```

Visualise the scatter plot matrix of the variables in *lrn14* to assess the distributions and correlations between the variables.
```{r}
# [-1] excludes the first column (gender). This because gender is not numerical
 pairs(lrn14[-1])

```

** Correlations between the variables in the dataframe**

Create a advanced plot matrix using *ggpairs()*. This matrix gives the correlation coefficient (R), the measures of strength and direction of a relationship between two variables on a scatterplot. 

The value of R lies between +1 (postive correlation) and –1( negative correlation).

A positive correlation implies as one variable increases the other one increases and if one decreases the other one decreases as well. 

A negative correlation means that the two variables under consideration vary in opposite directions, that is, if a variable increases the other decreases and vice versa.  

The strenght of the correlations is how close the R is to +1 for a strong positive result or how close to -1 for a strong negative correlation. 

However, a correlation close to 0 indicates no correlation and thus the two variables are independent.

```{r}
# access GGally using library
library(GGally)

# create a more advanced plot matrix with ggpairs()
p5 <- ggpairs(lrn14, mapping = aes(), lower = list(combo = wrap("facethist", bins = 20)))

# plot and take note of the correlations
p5

```


With Exam points as the target (dependent) variable.

We note that Exams points is positively correlated to Attitude(R= 0.44) and strategic learning (R= 0.15) but negatively correlated to surface learning (R= 0.14). Thus we include this in our multiple regresion model.

**Multiple Regression**

Multiple regression is a form of linear regression that permit predictions of the dependent variables with multiple independent variables i.e. the relationship between one dependent variable and two or more independent variables. This is done by basically adding more independent variables to the linear regression equation, with each term representing the impact of a different variable.

For our model with three independent variables we use the model below

   Y = β~0~ + β1~1~X~1~ + β1~2~X~2~ + β1~3~X~3~ + ε.

```{r}
# create a regression model with three explanatory variables (Note from the above matrix that shows correlation with Exam Points). Thus we have attutude, stra and surf

Model_1 <- lm(points ~ attitude+ stra+ surf, data = lrn14)

# print out a summary of the model
summary(Model_1)

#This is the output you should receive.
```
* Call: Shows the model formula and variables used to create the model.

* Residuals: Gives the difference between what the model predicted and the actual value of y.  

* Coefficients: Provide the weights that minimize the sum of the square of the errors.  

P-value is the significance level of the t-test (p-value = 3.156e-08), shows that the overall models including attitude, strategic learning and surface learning is statistically signficant in explaining 21% of variations in exams points at confidence level of 95%

However, individually, only attitude is significant (p-value = 1.93e-08) in explaning variation in exam points.

Thus, we only include attitude in our simple linear regression below

Formula: Y = β~0~ + β1~1~X~1~ + ε.

```{r}
# Example
# a scatter plot of Exam points versus attitude
qplot(attitude, points, data = lrn14) + geom_smooth(method = "lm")

# fit a linear model

Model_2 <- lm(points ~ attitude, data = lrn14)

# print out a summary of the model
summary(Model_2)

#This is the output you should receive.
```
In the above output, we can deduce that the estimate of the intercept β0 = 11.6372, and the estimate of the
slope β1 = 3.5255, 

Therefore, the line of best fit is

Exam points = 11.6372 + 3.5255 x Attitude

The R2 is 0.1906. 

R2 is a measure of the linear relationship between our predictor variable (Attitude) and our response (points). 

P-value is the significance level of the t-test (p-value = 4.119e-09), thus based on the result, Attitude is statistically signficant in explaining 19% of variations in exams points at confidence level of 95%


**Diagnostic Plots**

Regression diagnostics are used to evaluate the model assumptions.

The model assumptions for linear regression included:

1. Linearity: The relationship between independent variable and the mean of dependent variable is linear.

2. Homoscedasticity: The variance of residual is the same for any value of independent variable.

3. Independence: Observations are independent of each other.

4. Normality: For any fixed value of the independent variable, the dependent variable is normally distributed.

```{r}
# draw diagnostic plots using the plot() function. Choose the plots 1, 2 and 5
par(mfrow = c(2,2)) # Create a 2 x 2 plotting matrix
plot(Model_1, which= c(1, 2, 5))

```


1. Residuals vs Fitted

This plot shows if residuals have non-linear patterns. 

Residuals are measured as follows: residual =  observed y   –   model-predicted y

Therefore, this plot is useful for checking the assumption of linearity and homoscedasticity. 

 a. For the assumption of linearity, the residuals should not be too far away from 0.
 
 b. For the homoscedasticity assumption, we should not have a pattern in the residuals and that they are equally spread around the y = 0 line.

Based on this our plot, we can tell that the residual are even spread around the 0 line and no pattern can be seen. Therefore, our model  has met these assumption a good indication that we don’t have non-linear relationships.

2. Normal Q-Q

This plot shows if residuals are normally distributed. Therefore, we assess if the residuals follow a straight line well or deviate severely. 

Our data observations lie quite well along the 45-degree line in the QQ-plot, so we may assume that normality holds here as much as it divert at the begining.


3. Residuals vs Leverage

This plot helps us to find the influence of an outlier in the linear model. This is because some outliers may shift the linear regression analysis. While some may have outliers, that might not be affect the regression line. This is the case with our results.


























































































